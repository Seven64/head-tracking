# ğŸ¥ KI-Kopfverfolgung mit virtueller Kamera

Ein intelligentes Kopfverfolgungssystem, das Ihr Gesicht erkennt und eine virtuelle Kamera automatisch zoomt und ausrichtet. Perfekt fÃ¼r Video-Calls, Streaming und kreative Anwendungen!

## ğŸ¬ Demo-Video / Showdown

<video width="560" height="315" controls>
  <source src="videos/VCbhYCzgBF.mp4" type="video/mp4">
  Dein Browser unterstÃ¼tzt keine Video-Wiedergabe.
</video>


## âœ¨ Features

- **ğŸ¯ Automatische Gesichtserkennung** - Erkennt Ihr Gesicht in Echtzeit
- **ğŸ” Dynamischer Zoom** - Passt automatisch den Zoomfaktor an
- **ğŸ“ Intelligente Ausrichtung (Pan)** - Folgt dem Kopf sanft und ruckelfrei
- **ğŸ¬ Virtuelle Kamera** - Funktioniert mit Zoom, Teams, OBS und mehr
- **âš™ï¸ Konfigurierbar** - Alle Parameter leicht anpassbar
- **âœ… PEP 8 konform** - Sauberer, professioneller Code

## ğŸ–¥ï¸ Systemanforderungen

- **Python**: 3.8+
- **Betriebssystem**: Windows, macOS, Linux
- **Hardware**: 
  - Webcam / integrierte Kamera
  - Mindestens 4 GB RAM
  - Multi-Core Prozessor empfohlen

## ğŸ“¦ Installation

### Schritt 1: Python und pip installieren

Stelle sicher, dass Python 3.8+ installiert ist:
```bash
python --version
```

### Schritt 2: Repository klonen oder Dateien herunterladen

```bash
git clone https://github.com/dein-repo/head-tracking.git
cd head-tracking
```

### Schritt 3: AbhÃ¤ngigkeiten installieren

```bash
pip install -r requirements.txt
```

**BenÃ¶tigte Pakete:**
- `opencv-python` - Gesichtserkennung und Bildverarbeitung
- `numpy` - Numerische Berechnungen
- `pyvirtualcam` - Virtuelle Kamera

### Schritt 4: Programm ausfÃ¼hren

```bash
python head_tracking.py
```

## ğŸš€ Verwendung

### Grundlegende Bedienung

Das Programm startet automatisch mit Ihrer Standard-Webcam:

| Taste | Funktion |
|-------|----------|
| **V** | Visualisierung umschalten (zeigt erkannte Gesichter) |
| **Q** | Programm beenden |

### Beispiel-Session

```
$ python head_tracking.py
Virtual camera: OBS Camera
Press 'v' to toggle visualization
Press 'q' to quit
```

Das Programm wird nun:
1. âœ… Ihr Gesicht erkennen
2. âœ… Automatisch zoomen und ausrichten
3. âœ… Die virtuelle Kamera mit dem Output fÃ¼ttern

## âš™ï¸ Konfiguration

Ã–ffne `head_tracking.py` und passe diese Konstanten an:

```python
# Wie groÃŸ soll das Gesicht im Bild sein? (25% der Bildbreite)
IDEAL_FACE_WIDTH_RATIO = 0.25

# Zoom-Grenzen (verhindert zu extremes Verhalten)
MIN_ZOOM = 1.0    # Minimaler Zoom (kein Rauszoomen)
MAX_ZOOM = 2.5    # Maximaler Zoom (Pixelierung vermeiden)

# GlÃ¤ttungsfaktoren (kleinere = weichere Bewegung)
SMOOTHING_FACTOR_PAN = 0.1    # Kopfbewegung
SMOOTHING_FACTOR_ZOOM = 0.05  # Zoom-Bewegung
```

### Parameter erklÃ¤ren

| Parameter | Bereich | Effekt |
|-----------|---------|--------|
| `IDEAL_FACE_WIDTH_RATIO` | 0.1 - 0.5 | Kleinere Werte = weiter raus, mehr "Headroom" |
| `MAX_ZOOM` | 1.0 - 5.0 | HÃ¶here Werte = nÃ¤her heran, aber pixeliger |
| `SMOOTHING_FACTOR_PAN` | 0.01 - 0.3 | Kleinere = langsamere, glattere Kopfbewegung |
| `SMOOTHING_FACTOR_ZOOM` | 0.01 - 0.2 | Kleinere = sanfteres Zoomen |

## ğŸ” Wie es funktioniert

### Die Kopfverfolgung im Detail

```
1. GESICHTSERKENNUNG (Cascade Classifier)
   â†“
   Eingabebild â†’ Konvertierung zu Graustufen
   â†“
   Haar-Kaskade scannt nach Gesichtsmuster
   â†“
   GrÃ¶ÃŸtes erkanntes Gesicht wird ausgewÃ¤hlt

2. ZOOM-BERECHNUNG
   â†“
   Aktuelle Gesichtsbreite gemessen
   â†“
   Zoomfaktor = Ideal-Breite / Aktuelle-Breite
   â†“
   Zoom begrenzt auf MIN_ZOOM bis MAX_ZOOM

3. GLÃ„TTUNG (Smoothing)
   â†“
   Aktuelle Position + Ziel-Position blended
   â†“
   Formeln: new = old Ã— (1 - factor) + target Ã— factor
   â†“
   Ergebnis: ruckelfrei statt abrupt

4. BILDTRANSFORMATION
   â†“
   Rechteckiger Ausschnitt berechnet
   â†“
   Bild zuschneiden und hochskalieren
   â†“
   An virtuelle Kamera senden
```

### Beispiel: Smoothing erklÃ¤rt

Stellen Sie sich vor, der Zoomfaktor soll von 1.0 auf 2.0 gehen:

```python
# Ohne Smoothing (abrupt, ruckelig):
current_zoom = 2.0  # â† Sofort!

# Mit Smoothing (factor=0.05, weich):
current_zoom = 0.95 Ã— 1.0 + 0.05 Ã— 2.0 = 1.05  # Iteration 1
current_zoom = 0.95 Ã— 1.05 + 0.05 Ã— 2.0 = 1.0975  # Iteration 2
current_zoom = 0.95 Ã— 1.0975 + 0.05 Ã— 2.0 = 1.1426  # Iteration 3
# ... langsam sich annÃ¤hernd ...
```

Deshalb wirkt die Bewegung flÃ¼ssig und nicht ruckelig.

## ğŸ› HÃ¤ufige Probleme

### Problem: "Kamera kann nicht geÃ¶ffnet werden"

**Ursache**: Kamera nicht vorhanden oder nicht verfÃ¼gbar
```bash
# LÃ¶sung: Kamera-Index prÃ¼fen
# Ã„ndere in head_tracking.py:
cap = cv2.VideoCapture(1)  # statt 0
```

### Problem: Gesicht wird nicht erkannt

**Ursachen**:
- âŒ Schlechte Beleuchtung â†’ **Besseres Licht verwenden**
- âŒ Zu weit weg vom Bildschirm â†’ **NÃ¤her herangehen**
- âŒ Gesicht teilweise verdeckt â†’ **Behinderungen entfernen**

**LÃ¶sung**: Erkennung anpassen:
```python
# In process_frame():
faces = face_cascade.detectMultiScale(
    gray, 
    1.1,  # Scale factor (kleiner = prÃ¤ziser, langsamer)
    5     # Min neighbors (hÃ¶her = strengere Erkennung)
)
```

### Problem: Bewegung ist zu ruckelig/zu trÃ¤ge

**Zu trÃ¤ge** (verzÃ¶gertes Folgen):
- SMOOTHING_FACTOR erhÃ¶hen (z.B. 0.2)

**Zu ruckelig** (abrupte SprÃ¼nge):
- SMOOTHING_FACTOR senken (z.B. 0.02)

### Problem: Virtuelle Kamera wird nicht erkannt

**Windows**:
```bash
# OBS Virtual Camera installieren
# https://obsproject.com/
```

**macOS/Linux**:
```bash
pip install pyvirtualcam
# Die Virtualkamera sollte dann verfÃ¼gbar sein
```

## ğŸ“ Projektstruktur

```
head-tracking/
â”œâ”€â”€ head_tracking.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ DOKUMENTATION.md
â”œâ”€â”€ SETUP.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â””â”€â”€ config/
    â””â”€â”€ default_config.yaml
```

## ğŸ“ Technische HintergrÃ¼nde

### Cascade Classifier (Haar-Kaskade)

Das Programm nutzt OpenCV's vortrainiertes Modell:
```python
haarcascade_frontalface_default.xml
```

Dies ist ein **Machine Learning Modell**, das Gesichtsmuster erkennt:
- âœ… Schnell (Echtzeit mÃ¶glich)
- âœ… ZuverlÃ¤ssig unter guten Bedingungen
- âŒ Weniger genau bei extremen Winkeln
- âŒ Braucht gute Beleuchtung

### ROI (Region of Interest)

Nur ein kleiner Bereich des Bildes wird verarbeitet:
```python
roi_x1 = int(np.clip(face_center[0] - zoom_width / 2, 0, width - zoom_width))
roi_y1 = int(np.clip(face_center[1] - zoom_height / 2, 0, height - zoom_height))
```

Dies ist eine **Ausschneideoperation**, die:
- ğŸ”º CPU-Ressourcen spart
- ğŸ”º BildqualitÃ¤t verbessert
- ğŸ”º Zoom-Effekt erzeugt

## ğŸ“‹ Lizenz

Dieses Projekt ist Open Source und kann frei verwendet werden.

## ğŸ¤ Beitragen

Dein Feedback und deine Verbesserungen sind willkommen!

### Pull Requests

1. Fork das Projekt
2. Feature-Branch erstellen: `git checkout -b feature/neue-funktion`
3. Ã„nderungen committen: `git commit -m "Neue Funktion hinzugefÃ¼gt"`
4. Push zum Branch: `git push origin feature/neue-funktion`
5. Pull Request Ã¶ffnen

### Code-Standard

Dieses Projekt folgt **PEP 8**:
```bash
pip install flake8
flake8 head_tracking.py
```

## ğŸ“ Support

Bei Fragen oder Problemen:
- ğŸ“§ Issues auf GitHub Ã¶ffnen
- ğŸ’¬ Diskussionen starten
- ğŸ“– Dokumentation lesen

---

**Viel SpaÃŸ mit dem Kopfverfolgungssystem!** ğŸ¬âœ¨
